第三章学习

3.1 思考

在机器学习中，进行机器学习的步骤，首先需要数据预处理，将数据处理成为能够进行训练的形式，例如大模型的模型训练前需要对数据清洗，转换成大模型能够读取的格式。

在深度学习中同样如此。对数据清洗处理后，划分训练集和测试集，选择基模，设定损失函数，优化器，超参等等。深度学习的网络需要逐层搭建，同时按批次输入数据，将数据放入GPU中

3.2 基本配置

需要学习到一些基本的超参数，batch size，由于在深度学习中，数据是按照batch进行读取，前向传播计算loss然后反向传播更新参数，但是batch size的数量不能过大，gpu承载能力有限，也不能过小，训练时间很长。学习率的策略，决定了参数更新的步长。epoch是遍历整个数据集的次数，过大会导致模型过拟合。

3.3 指定gpu的方式

我在用vllm部署模型时，常常用到的命令是这样

```python
nohup env CUDA_VISIBLE_DEVICES=0,1,2,3
python -m vllm...
```

这种和设置操作系统环境变量很像。当然也可以使用框架内置的device对象，之后对要使用GPU的变量用.to(device)即可，那么哪些变量会使用这样的方法呢？为什么要使用GPU呢？

**总结：所有参与神经网络前向传播，损失计算和反向传播的tensor张量，都需要被移动到相同的GPU设备上**

3.4 模型构建

**M**ulti-Layer **P**erceptron 简称MLP，多层感知机，是神经网络最基础的模型。需·要清楚的是，每一层输入和输出的维度信息，每个模型有多少个hidden layer。

```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```

资料中MLP构造了两个隐藏层，一个用于线性变换，一个relu函数。但是当我们打印参数的size时，发现隐藏层的weight形状是torch.Size([256, 784])。

这是因为 Y=X*WT+B ，X的形状是(N, 784)，N是批次的大小，W是权重矩阵，形状是torch.Size([256, 784])，进行转置后可以和X相乘，bias的形状是torch.Size([256])，这样就可以保证hidden层的输出维度是784。

我们发现，执行net(X)，会执行forward函数，这是因为net(X)会调用Module类的__call__函数，这个函数会调用模型类定义的forward函数完成前向计算，因此这个函数需要我们提前写好。

对于神经网络中的层，有的含有模型参数，这些会被优化器更新。

```python
self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
self.params.append(nn.Parameter(torch.randn(4, 1)))
```

以上就是为层定义参数，这发生在初始化模型的方法中。

CNN

- **二维卷积层**

  在实际的深度学习中，卷积层处理的张量通常是四维的：

  - `N`：批量大小 (Batch Size)，一次处理的样本数量。
  - `C_in`：输入通道数 (Input Channels)，例如彩色图像有 3 个通道 (R, G, B)，灰度图像有 1 个通道。
  - `H_in`：输入高度 (Input Height)。
  - `W_in`：输入宽度 (Input Width)。

  所以，一个典型的输入张量形状是 `(N, C_in, H_in, W_in)`。 卷积层的输出张量形状通常是 `(N, C_out, H_out, W_out)`。
  - X = X.view((1, 1) + X.shape)：这行代码的核心是利用了 Python 中**元组（tuple）的拼接**特性：`X.shape` 是一个 `torch.Size` 对象，它表现得像一个元组，在 Python 中，可以用 `+` 运算符来拼接两个元组。因此，如果 `X.shape` 是 `torch.Size([8, 8])` (等同于元组 `(8, 8)`) 那么 `(1, 1) + X.shape` 就会变成 `(1, 1) + (8, 8)` 拼接后的结果是一个新的元组：**(1, 1, 8, 8)**

```python
# 实例化PyTorch 的 nn 模块中预定义的 Conv2d 类
conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)
```

**填充（padding）可以精确抵消了卷积核对尺寸的缩小。**

**输出尺寸计算公式：** 

Output=⌊(Input+2×Padding−Kernel)/Stride⌋+1 

- **高度：** 输入 8，卷积核 5。为保持 8 不变，需 Padding=(5−1)/2=2。 计算：(8+2×2−5)/1+1=8。
- **宽度：** 输入 8，卷积核 3。为保持 8 不变，需 Padding=(3−1)/2=1。 计算：(8+2×1−3)/1+1=8。

因此，当卷积核尺寸为奇数且步幅为 1 时，设置 `Padding = (Kernel_Size - 1) / 2` 即可使输出尺寸与输入一致。

​	卷积层本质是从输入数据中**学习并提取多层次抽象特征**。它通过一个可学习的、二维（或多维）的“卷积核”在输入特征图上进行滑动操作（即互相关运算），在每个位置对局部区域执行加权求和，从而捕获诸如边缘、纹理、颜色、形状等不同尺度的信息。这一过程不仅有效减少了模型参数量，提升了模型对输入数据平移、旋转等微小形变的鲁棒性，还能够通过堆叠多层卷积网络，逐步从低级特征构建出更高级、更具语义的抽象特征表示。

- **池化层**

  卷积层之后，池化层在卷积神经网络中扮演着关键的**特征降维与不变性增强**的角色。它通常不包含可学习参数，而是通过对输入特征图的局部区域执行预定义的汇总操作（如最大池化或平均池化）来实现下采样。最大池化通过提取每个池化窗口内的最大激活值，能够有效捕获区域内的最显著特征，并对特征位置的微小变化产生一定的不变性；而平均池化则通过计算窗口内的平均值，保留区域的整体背景信息。通过这种方式，池化层显著减少了特征图的空间维度，从而降低了后续网络层的计算复杂度和参数数量，有效控制了模型的过拟合风险，并提高了模型处理不同尺寸输入的能力，使得网络能够更高效、更鲁棒地进行学习和预测。

- 一些规定

  在深度学习中，为了训练效率和稳定性，我们通常不会一次只给网络看一个样本，而是会同时看一小批（通常几十到几百个）样本。这个“一小批”就是**小批量（mini-batch）**，如果是一个单独的样本，只需要使用 `input.unsqueeze(0)` 来添加一个“假的”批大小维度。例如，你有一张灰度图，它的形状是 `(C, H, W)`，也就是 `(1, 28, 28)`。如果直接把它输入给 `nn.Conv2d`，会报错，因为它缺少批量维度。通过 `input.unsqueeze(0)`，你就在最前面（索引 0 的位置）增加了一个维度 1。那么 `(1, 28, 28)` 就会变成 `(1, 1, 28, 28)`。

  现在它的形状就是 `(批量大小=1, 通道数=1, 高度=28, 宽度=28)`，符合 `nn.Conv2d` 的要求了。

模型示例：

AlexNet 是深度学习发展史上的一个里程碑，它在 2012 年赢得了 ImageNet 大赛，证明了深度卷积神经网络在图像识别任务上的强大能力。

3.5 模型初始化

```python
# 3.2 重塑以适应模型层 (例如：注意力机制)
# 场景：LLM中的多头注意力机制，可能需要将 (batch_size, seq_len, embed_dim)
# 重塑为 (batch_size, num_heads, seq_len, head_dim)。
# 假设 original_data 是 (2, 4, 3)，我们要模拟一个 (batch_size, seq_len, num_heads, head_dim) 的中间态。
# 这里简化示例，假设 embed_dim (3) 可以拆分为 num_heads (1) * head_dim (3)
# 实际LLM中 embed_dim = num_heads * head_dim

# 假设嵌入维度 3 可以被分解为 1个头，每个头维度3
# reshaped_for_attention = original_data.view(original_data.shape[0], original_data.shape[1], 1, 3)
# print(f"中间重塑形状 (批次, 序列, 头数, 头维度): {reshaped_for_attention.shape}") # torch.Size([2, 4, 1, 3])

# 更真实的LLM注意力重塑（假设原始嵌入维度为512，8个头，每个头64维）
# batch_tensor = torch.randn(32, 10, 512)
# reshaped_for_attn = batch_tensor.view(32, 10, 8, 64) # (batch, seq_len, num_heads, head_dim)
# print(f"模拟注意力重塑前: {batch_tensor.shape}")
# print(f"模拟注意力重塑后 (view): {reshaped_for_attn.shape}")
# 通常还需要 permute (调换维度顺序) 来满足注意力层的输入要求：
# final_attn_input = reshaped_for_attn.permute(0, 2, 1, 3) # (batch, num_heads, seq_len, head_dim)
# print(f"模拟注意力最终输入形状 (permute): {final_attn_input.shape}\n")


# --- 4. 确保张量数据独立性：clone() 与 view() 的结合 ---
# 在LLM中，如果需要对中间结果进行不同分支的计算，且不想互相影响，clone()至关重要。

print("\n--- 4. 确保张量数据独立性：clone() 与 view() 的结合 ---")

# 原始张量
source_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
print(f"原始张量:\n{source_tensor}")

# 4.1 直接使用 view() (共享内存)
# 模拟LLM中某个模块的输出，直接view到另一个形状
viewed_tensor = source_tensor.view(-1) # 展平为一维
print(f"直接view后的张量:\n{viewed_tensor}")

# 尝试修改viewed_tensor的第一个元素
viewed_tensor[0] = 99.0
print(f"修改viewed_tensor后:\n{viewed_tensor}")
print(f"原始张量 (受到影响):\n{source_tensor}\n") # 原始张量也被修改了，因为它们共享内存

# 4.2 使用 clone().view() (创建独立副本)
# 在LLM中，例如，一个中间特征可能被用于多个并行子模块，每个子模块需要独立处理
source_tensor_original = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # 重新初始化原始张量
print(f"重新初始化的原始张量:\n{source_tensor_original}")

# 先克隆，再进行view操作
cloned_then_viewed_tensor = source_tensor_original.clone().view(-1)
print(f"先clone再view后的张量:\n{cloned_then_viewed_tensor}")

# 尝试修改 cloned_then_viewed_tensor 的第一个元素
cloned_then_viewed_tensor[0] = 99.0
print(f"修改cloned_then_viewed_tensor后:\n{cloned_then_viewed_tensor}")
print(f"原始张量 (未受影响):\n{source_tensor_original}\n") # 原始张量不受影响，因为是独立副本

# --- 5. 单元素张量取值：item() ---
# 在LLM训练或推理中，我们经常需要从计算结果（如损失值或某个标量预测值）中提取纯Python数值。

print("\n--- 5. 单元素张量取值：item() ---")

# 模拟LLM的某个输出，例如一个计算出的损失值
loss_tensor = torch.tensor(0.00123)
print(f"损失张量: {loss_tensor}")
print(f"损失张量的类型: {type(loss_tensor)}")

# 使用 .item() 提取纯数值
loss_value = loss_tensor.item()
print(f"提取后的损失值: {loss_value}")
print(f"提取后损失值的类型: {type(loss_value)}")

# 另一个例子：LLM的预测概率值
single_prediction_score = torch.tensor([0.95])
print(f"单元素预测分数张量: {single_prediction_score}")
print(f"提取后的预测分数: {single_prediction_score.item()}\n")



class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))
    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)
```

