第三章学习

3.1 思考

在机器学习中，进行机器学习的步骤，首先需要数据预处理，将数据处理成为能够进行训练的形式，例如大模型的模型训练前需要对数据清洗，转换成大模型能够读取的格式。

在深度学习中同样如此。对数据清洗处理后，划分训练集和测试集，选择基模，设定损失函数，优化器，超参等等。深度学习的网络需要逐层搭建，同时按批次输入数据，将数据放入GPU中

3.2 基本配置

需要学习到一些基本的超参数，batch size，由于在深度学习中，数据是按照batch进行读取，前向传播计算loss然后反向传播更新参数，但是batch size的数量不能过大，gpu承载能力有限，也不能过小，训练时间很长。学习率的策略，决定了参数更新的步长。epoch是遍历整个数据集的次数，过大会导致模型过拟合。

3.3 指定gpu的方式

我在用vllm部署模型时，常常用到的命令是这样

```python
nohup env CUDA_VISIBLE_DEVICES=0,1,2,3
python -m vllm...
```

这种和设置操作系统环境变量很像。当然也可以使用框架内置的device对象，之后对要使用GPU的变量用.to(device)即可，那么哪些变量会使用这样的方法呢？为什么要使用GPU呢？

**总结：所有参与神经网络前向传播，损失计算和反向传播的tensor张量，都需要被移动到相同的GPU设备上**

3.4 模型构建

**M**ulti-Layer **P**erceptron 简称MLP，多层感知机，是神经网络最基础的模型。需·要清楚的是，每一层输入和输出的维度信息，每个模型有多少个hidden layer。

```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```

资料中MLP构造了两个隐藏层，一个用于线性变换，一个relu函数。但是当我们打印参数的size时，发现隐藏层的weight形状是torch.Size([256, 784])。

这是因为 Y=X*WT+B ，X的形状是(N, 784)，N是批次的大小，W是权重矩阵，形状是torch.Size([256, 784])，进行转置后可以和X相乘，bias的形状是torch.Size([256])，这样就可以保证hidden层的输出维度是784。

我们发现，执行net(X)，会执行forward函数，这是因为net(X)会调用Module类的__call__函数，这个函数会调用模型类定义的forward函数完成前向计算，因此这个函数需要我们提前写好。

对于神经网络中的层，有的含有模型参数，这些会被优化器更新。

```python
self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
self.params.append(nn.Parameter(torch.randn(4, 1)))
```

以上就是为层定义参数，这发生在初始化模型的方法中。

CNN

- **二维卷积层**

  在实际的深度学习中，卷积层处理的张量通常是四维的：

  - `N`：批量大小 (Batch Size)，一次处理的样本数量。
  - `C_in`：输入通道数 (Input Channels)，例如彩色图像有 3 个通道 (R, G, B)，灰度图像有 1 个通道。
  - `H_in`：输入高度 (Input Height)。
  - `W_in`：输入宽度 (Input Width)。

  所以，一个典型的输入张量形状是 `(N, C_in, H_in, W_in)`。 卷积层的输出张量形状通常是 `(N, C_out, H_out, W_out)`。
  - X = X.view((1, 1) + X.shape)：这行代码的核心是利用了 Python 中**元组（tuple）的拼接**特性：`X.shape` 是一个 `torch.Size` 对象，它表现得像一个元组，在 Python 中，可以用 `+` 运算符来拼接两个元组。因此，如果 `X.shape` 是 `torch.Size([8, 8])` (等同于元组 `(8, 8)`) 那么 `(1, 1) + X.shape` 就会变成 `(1, 1) + (8, 8)` 拼接后的结果是一个新的元组：**(1, 1, 8, 8)**

```python
# 实例化PyTorch 的 nn 模块中预定义的 Conv2d 类
conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)
```

**填充（padding）可以精确抵消了卷积核对尺寸的缩小。**

**输出尺寸计算公式：** 

Output=⌊(Input+2×Padding−Kernel)/Stride⌋+1 

- **高度：** 输入 8，卷积核 5。为保持 8 不变，需 Padding=(5−1)/2=2。 计算：(8+2×2−5)/1+1=8。
- **宽度：** 输入 8，卷积核 3。为保持 8 不变，需 Padding=(3−1)/2=1。 计算：(8+2×1−3)/1+1=8。

因此，当卷积核尺寸为奇数且步幅为 1 时，设置 `Padding = (Kernel_Size - 1) / 2` 即可使输出尺寸与输入一致。

​	卷积层本质是从输入数据中**学习并提取多层次抽象特征**。它通过一个可学习的、二维（或多维）的“卷积核”在输入特征图上进行滑动操作（即互相关运算），在每个位置对局部区域执行加权求和，从而捕获诸如边缘、纹理、颜色、形状等不同尺度的信息。这一过程不仅有效减少了模型参数量，提升了模型对输入数据平移、旋转等微小形变的鲁棒性，还能够通过堆叠多层卷积网络，逐步从低级特征构建出更高级、更具语义的抽象特征表示。

- **池化层**

  卷积层之后，池化层在卷积神经网络中扮演着关键的**特征降维与不变性增强**的角色。它通常不包含可学习参数，而是通过对输入特征图的局部区域执行预定义的汇总操作（如最大池化或平均池化）来实现下采样。最大池化通过提取每个池化窗口内的最大激活值，能够有效捕获区域内的最显著特征，并对特征位置的微小变化产生一定的不变性；而平均池化则通过计算窗口内的平均值，保留区域的整体背景信息。通过这种方式，池化层显著减少了特征图的空间维度，从而降低了后续网络层的计算复杂度和参数数量，有效控制了模型的过拟合风险，并提高了模型处理不同尺寸输入的能力，使得网络能够更高效、更鲁棒地进行学习和预测。

- 一些规定

  在深度学习中，为了训练效率和稳定性，我们通常不会一次只给网络看一个样本，而是会同时看一小批（通常几十到几百个）样本。这个“一小批”就是**小批量（mini-batch）**，如果是一个单独的样本，只需要使用 `input.unsqueeze(0)` 来添加一个“假的”批大小维度。例如，你有一张灰度图，它的形状是 `(C, H, W)`，也就是 `(1, 28, 28)`。如果直接把它输入给 `nn.Conv2d`，会报错，因为它缺少批量维度。通过 `input.unsqueeze(0)`，你就在最前面（索引 0 的位置）增加了一个维度 1。那么 `(1, 28, 28)` 就会变成 `(1, 1, 28, 28)`。

  现在它的形状就是 `(批量大小=1, 通道数=1, 高度=28, 宽度=28)`，符合 `nn.Conv2d` 的要求了。

模型示例：

AlexNet 是深度学习发展史上的一个里程碑，它在 2012 年赢得了 ImageNet 大赛，证明了深度卷积神经网络在图像识别任务上的强大能力。

3.5 模型初始化
