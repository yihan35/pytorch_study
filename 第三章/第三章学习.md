第三章学习



```python
# 3.2 重塑以适应模型层 (例如：注意力机制)
# 场景：LLM中的多头注意力机制，可能需要将 (batch_size, seq_len, embed_dim)
# 重塑为 (batch_size, num_heads, seq_len, head_dim)。
# 假设 original_data 是 (2, 4, 3)，我们要模拟一个 (batch_size, seq_len, num_heads, head_dim) 的中间态。
# 这里简化示例，假设 embed_dim (3) 可以拆分为 num_heads (1) * head_dim (3)
# 实际LLM中 embed_dim = num_heads * head_dim

# 假设嵌入维度 3 可以被分解为 1个头，每个头维度3
# reshaped_for_attention = original_data.view(original_data.shape[0], original_data.shape[1], 1, 3)
# print(f"中间重塑形状 (批次, 序列, 头数, 头维度): {reshaped_for_attention.shape}") # torch.Size([2, 4, 1, 3])

# 更真实的LLM注意力重塑（假设原始嵌入维度为512，8个头，每个头64维）
# batch_tensor = torch.randn(32, 10, 512)
# reshaped_for_attn = batch_tensor.view(32, 10, 8, 64) # (batch, seq_len, num_heads, head_dim)
# print(f"模拟注意力重塑前: {batch_tensor.shape}")
# print(f"模拟注意力重塑后 (view): {reshaped_for_attn.shape}")
# 通常还需要 permute (调换维度顺序) 来满足注意力层的输入要求：
# final_attn_input = reshaped_for_attn.permute(0, 2, 1, 3) # (batch, num_heads, seq_len, head_dim)
# print(f"模拟注意力最终输入形状 (permute): {final_attn_input.shape}\n")


# --- 4. 确保张量数据独立性：clone() 与 view() 的结合 ---
# 在LLM中，如果需要对中间结果进行不同分支的计算，且不想互相影响，clone()至关重要。

print("\n--- 4. 确保张量数据独立性：clone() 与 view() 的结合 ---")

# 原始张量
source_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
print(f"原始张量:\n{source_tensor}")

# 4.1 直接使用 view() (共享内存)
# 模拟LLM中某个模块的输出，直接view到另一个形状
viewed_tensor = source_tensor.view(-1) # 展平为一维
print(f"直接view后的张量:\n{viewed_tensor}")

# 尝试修改viewed_tensor的第一个元素
viewed_tensor[0] = 99.0
print(f"修改viewed_tensor后:\n{viewed_tensor}")
print(f"原始张量 (受到影响):\n{source_tensor}\n") # 原始张量也被修改了，因为它们共享内存

# 4.2 使用 clone().view() (创建独立副本)
# 在LLM中，例如，一个中间特征可能被用于多个并行子模块，每个子模块需要独立处理
source_tensor_original = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # 重新初始化原始张量
print(f"重新初始化的原始张量:\n{source_tensor_original}")

# 先克隆，再进行view操作
cloned_then_viewed_tensor = source_tensor_original.clone().view(-1)
print(f"先clone再view后的张量:\n{cloned_then_viewed_tensor}")

# 尝试修改 cloned_then_viewed_tensor 的第一个元素
cloned_then_viewed_tensor[0] = 99.0
print(f"修改cloned_then_viewed_tensor后:\n{cloned_then_viewed_tensor}")
print(f"原始张量 (未受影响):\n{source_tensor_original}\n") # 原始张量不受影响，因为是独立副本

# --- 5. 单元素张量取值：item() ---
# 在LLM训练或推理中，我们经常需要从计算结果（如损失值或某个标量预测值）中提取纯Python数值。

print("\n--- 5. 单元素张量取值：item() ---")

# 模拟LLM的某个输出，例如一个计算出的损失值
loss_tensor = torch.tensor(0.00123)
print(f"损失张量: {loss_tensor}")
print(f"损失张量的类型: {type(loss_tensor)}")

# 使用 .item() 提取纯数值
loss_value = loss_tensor.item()
print(f"提取后的损失值: {loss_value}")
print(f"提取后损失值的类型: {type(loss_value)}")

# 另一个例子：LLM的预测概率值
single_prediction_score = torch.tensor([0.95])
print(f"单元素预测分数张量: {single_prediction_score}")
print(f"提取后的预测分数: {single_prediction_score.item()}\n")
```

