第二章：PyTorch 中张量（Tensor）的理解与基础操作

本章内容

**一、张量的理解**

张量可以被视为向量和矩阵在高维空间的推广。它是一种能够统一表示标量、向量、矩阵以及更高维度数据结构的数学概念。

- **0维张量（Scalar）：** 代表一个单一的数值，不具备方向性，如同物理量中的标量（例如：温度、年龄）。
- **1维张量（Vector）：** 代表一个具有大小和方向的量，通常表现为一串有序的数字序列（例如：坐标、特征向量）。
- **2维张量（Matrix）：** 表现为一个二维的数值表格，具有行和列的结构，广泛应用于表示数据集（例如：考试成绩单、灰度图像）。
- **3维及更高维张量：** 用于表示更为复杂的数据结构。
  - **3维张量示例：** 彩色图像（通常由宽度、高度和RGB颜色通道构成），时间序列数据（例如：批量的传感器读数，可以看作是时间步、特征维度和批次大小的组合）。
  - **更高维示例：** 视频数据可以被视为4维张量（帧数、宽度、高度、颜色通道）。

**二、PyTorch 中的 `torch.Tensor`：核心数据结构**

在 PyTorch 框架中，`torch.Tensor` 是其最核心的数据结构，用于存储和操作数据。它在功能上与 NumPy 的 `ndarray`（多维数组）非常相似，但在深度学习场景中具备显著优势：

1. **GPU 加速计算能力：** `torch.Tensor` 能够无缝地在 CPU 和 GPU 之间切换执行计算。GPU 擅长并行处理大量数据，这对于深度学习模型训练中涉及的海量浮点运算至关重要，能极大提升计算效率。
2. **自动求梯度（Automatic Differentiation）：** 这是 `torch.Tensor` 最强大的特性之一。在模型训练过程中，我们需要计算损失函数对模型参数的梯度，以便进行优化（如梯度下降）。`torch.Tensor` 能够自动记录所有操作，并构建计算图，从而自动完成梯度的计算，极大地简化了深度学习模型的开发和调试。

这两个特性使得 `torch.Tensor` 成为构建和训练神经网络的理想选择。

**三、张量的创建与属性获取**

PyTorch 提供了多种方法来创建张量：

- **`torch.ones(shape, dtype)`：** 创建指定形状（`shape`）且所有元素都为1的张量。`dtype` 参数用于指定数据类型，如 `torch.double` (双精度浮点数) 或 `torch.float` (单精度浮点数)。
- **`torch.randn_like(input_tensor, dtype)`：** 创建一个与 `input_tensor` 形状相同，但元素为随机标准正态分布（均值为0，方差为1）的张量。同样可以指定数据类型。
- **`torch.arange(start, end)`：** 创建一个从 `start` 开始（包含 `start`）到 `end` 之前（不包含 `end`）的等差数列张量。

获取张量属性：

- **`tensor.size()` 和 `tensor.shape`：** 两者功能相同，都返回张量的形状信息，即每个维度的大小（例如：`(4, 3)` 表示4行3列）。

**四、张量的索引与切片**

张量的索引和切片操作与 Python 列表或 NumPy 数组类似，允许我们访问张量中的特定元素或子区域。

- **`tensor[row_index, col_index]`：** 用于访问特定位置的元素。
- **`tensor[:, col_index]`：** `:` 冒号表示选取该维度的所有元素。例如，`x[:, 1]` 表示选取张量 `x` 的所有行（`:`），以及索引为 `1` 的列（即第二列）。==逗号前表示行，逗号后表示列，：表示所有。==

**五、张量的维度变换：`view()` 与 `reshape()`**

维度变换指的是不改变张量中元素总数的前提下，改变其形状（即维度的排列方式）。

1. **`tensor.view(new_shape)`：**
   - **功能：** 改变张量的形状。可以指定具体的维度大小，也可以使用 `-1` 让 PyTorch 自动推断该维度的大小（例如：`x.view(16)` 将4x4的张量展平为一维的16个元素；`x.view(-1, 8)` 会将张量重新排列为 `N` 行8列，其中 `N` 由总元素数决定）。
   - **特性：** `view()` 返回的新张量与原张量**共享底层内存**。这意味着，对新张量的修改会直接反映到原张量上，反之亦然。`view` 仅仅是改变了对同一块内存数据的“观察视角”。
2. **`tensor.reshape(new_shape)`：**
   - **功能：** 同样用于改变张量形状。
   - **特性：** `reshape()` 的行为不如 `view()` 稳定。它可能返回一个视图（共享内存），也可能返回一个拷贝（不共享内存），这取决于原张量在内存中的存储是否连续。因此，官方**不推荐**直接使用 `reshape()` 来确保不共享内存。

**六、确保张量数据独立性：`clone()` 与 `view()` 的结合**

在许多场景下，我们希望对张量进行形状变换时，新张量与原张量是完全独立的，互不影响。为了实现这一点，推荐的做法是：

- **`tensor.clone()`：** 先创建原张量的一个**独立副本**（即深拷贝，不共享内存）。`clone()` 操作本身会被记录在计算图中，这意味着在反向传播时，梯度可以从副本回传到原始张量，保持了计算的连续性。
- **再对克隆后的张量使用 `view()` 进行维度变换。** 这样，对新张量的任何修改都不会影响到原始张量。

**七、单元素张量取值：`item()`**

当一个张量中只包含一个元素时（例如：`tensor([[10]])`），我们可以使用 `.item()` 方法来提取其标量值。这会返回一个标准的 Python 数值类型，而非一个张量对象，方便后续的数学运算或与其他库的交互。



内容扩展

```python
import torch

# --- 1. 文本数据的张量表示 ---
# 在大语言模型 (LLM) 中，所有文本都必须转化为数值形式才能被计算机处理。

print("--- 1. 文本数据的张量表示 ---")

# 1.1 单个词/Token 的嵌入向量 (1维张量)
# 假设每个词被映射成一个512维的向量，这个向量代表词的语义信息。
# 比如，“LLM”这个词的嵌入向量。
# 对应：1维张量 (Vector)
embedding_llm = torch.tensor([0.1, -0.5, 0.8, 0.2, 0.9]) # 实际维度会大很多，这里简化为5维
print(f"单个词嵌入张量 (1D): {embedding_llm}")
print(f"形状: {embedding_llm.shape}\n")

# 1.2 单个句子/序列的表示 (2维张量/矩阵)
# 一个句子由多个词组成。假设一个句子有 3 个词，每个词是 5 维向量。
# 对应：2维张量 (Matrix)
# torch.randn 用于生成服从标准正态分布的随机数
sentence_tensor = torch.randn(3, 5) # 3个词，每个词5维
print(f"单个句子张量 (2D - 序列长度 x 嵌入维度):\n{sentence_tensor}")
print(f"形状: {sentence_tensor.shape}\n")

# 1.3 一个批次 (Batch) 的句子表示 (3维张量)
# LLM 训练时通常批量处理句子，以提高效率。
# 假设一个批次有 2 个句子，每个句子最长是 3 个词，每个词是 5 维向量。
# 对应：3维张量
batch_tensor = torch.randn(2, 3, 5) # 批次大小2, 序列长度3, 嵌入维度5
print(f"一个批次的句子张量 (3D - 批次大小 x 序列长度 x 嵌入维度):\n{batch_tensor}")
print(f"形状: {batch_tensor.shape}\n")


# --- 2. torch.Tensor 的超能力在 LLM 中的体现 ---
# GPU 计算和自动求梯度是LLM训练的基石。

print("\n--- 2. torch.Tensor 的超能力在 LLM 中的体现 ---")

# 2.1 GPU 计算 (GPU Computing)
# 检查是否有可用的GPU (CUDA设备)
if torch.cuda.is_available():
    print("CUDA (GPU) 可用！")
    # 创建一个在CPU上的张量
    data_on_cpu = torch.randn(100, 100, 512) # 模拟一个相对较大的张量
    print(f"张量初始设备: {data_on_cpu.device}")

    # 将张量移动到GPU上进行计算，这将大大加速LLM的训练过程
    data_on_gpu = data_on_cpu.to('cuda')
    print(f"张量移动到GPU后的设备: {data_on_gpu.device}")

    # 模拟在GPU上进行一次运算 (例如，矩阵乘法)
    # 在LLM中，大量的矩阵乘法是其核心运算
    result_on_gpu = torch.matmul(data_on_gpu, torch.randn(512, 256, device='cuda'))
    print(f"GPU上运算结果的设备: {result_on_gpu.device}\n")
else:
    print("CUDA (GPU) 不可用。所有计算将在CPU上进行，这会慢很多。\n")


# 2.2 自动求梯度 (Automatic Differentiation)
# 这是LLM训练的核心机制，无需手动计算复杂的梯度。
print("--- 2.2 自动求梯度示例 (LLM 训练核心) ---")

# 模拟LLM中的一个可学习参数（例如：嵌入层的权重）
# requires_grad=True 表示PyTorch需要为这个张量计算梯度
model_param = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"模型参数: {model_param}")

# 模拟一个简单的计算过程 (LLM内部的复杂运算最终都会转化为这种张量操作)
# output = model_param * 2
intermediate_output = model_param * 2
print(f"中间输出: {intermediate_output}")

# 模拟一个损失函数 (通常是模型预测与真实值之间的差距)
# 假设我们希望 intermediate_output 最终都是10
loss = (intermediate_output - 10).pow(2).sum() # 损失函数：平方和
print(f"计算得到的损失: {loss.item()}") # .item() 用于提取单元素张量的Python数值

# 执行反向传播，自动计算梯度
# loss.backward() 会计算损失对所有 requires_grad=True 的张量的梯度
loss.backward()

# 梯度现在存储在 model_param.grad 中
# 这个梯度指示了 model_param 应该如何调整才能使损失减小
print(f"模型参数的梯度: {model_param.grad}\n")
# 这里的梯度 [ -18.0, -16.0, -14.0] 表示：
# 如果我们沿着梯度的负方向更新 model_param (即 model_param - learning_rate * model_param.grad),
# 损失将会降低。


# --- 3. 维度变换：view() / reshape() 在 LLM 中的应用 ---
# 在LLM的模型架构中，经常需要改变张量的形状以适应不同层的输入输出。

print("\n--- 3. 维度变换：view() / reshape() ---")

# 假设一个批次的句子张量 (2个句子，每个句子4个词，每个词3维)
original_data = torch.randn(2, 4, 3)
print(f"原始张量形状: {original_data.shape}") # torch.Size([2, 4, 3])

# 3.1 展平操作 (Flattening)
# 场景：在将文本特征输入到全连接层之前，常需要将多维数据展平为一维。
# 例如，将每个句子 (4个词 x 3维/词 = 12维) 展平。
# -1 会自动推断维度大小 (这里是 4 * 3 = 12)
flattened_data = original_data.view(original_data.shape[0], -1)
print(f"展平后的形状 (批次大小 x 展平后的特征维度): {flattened_data.shape}") # torch.Size([2, 12])
# print(f"展平后的数据:\n{flattened_data}\n")
```

对于展平的代码执行：

![image-20250721130528576](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250721130528576.png)